{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introduction to Tensorflow.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "LpULn95mVNYF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Introduction To TensorFlow\n",
        "\n",
        "## Why TensorFlow?\n",
        "\n",
        "The entire purpose of TensorFlow is to have a so-called computational graph that can be executed much more efficiently than if the same calculations were to be performed directly in Python. TensorFlow can be more efficient than NumPy because TensorFlow knows the entire computation graph that must be executed, while NumPy only knows the computation of a single mathematical operation at a time.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "3p0RR3PEZ3E8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Simple-graph-example-260x300.png\" width=\"320\" height=\"300\" align=\"center\"/></center>"
      ]
    },
    {
      "metadata": {
        "id": "TpWd4M3ZWP8W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This may seem like a silly example – but notice a powerful idea in expressing the equation this way: two of the computations $(d=b+c$  and $ e=c+2)$ can be performed in parallel.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "1dG0vZw9Vz-n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TensorFlow can also automatically calculate the gradients that are needed to optimize the variables of the graph so as to make the model perform better. This is because the graph is a combination of simple mathematical expressions so the gradient of the entire graph can be calculated using the chain-rule for derivatives.\n",
        "\n",
        "TensorFlow can also take advantage of multi-core CPUs as well as GPUs - and Google has even built special chips just for TensorFlow which are called TPUs (Tensor Processing Units) that are even faster than GPUs.\n",
        "\n",
        "A TensorFlow graph consists of the following parts which will be detailed below:\n",
        "\n",
        "* Placeholder variables used to feed input into the graph.\n",
        "* Model variables that are going to be optimized so as to make the model perform better.\n",
        "* The model which is essentially just a mathematical function that calculates some output given the input in the placeholder variables and the model variables.\n",
        "* A cost measure that can be used to guide the optimization of the variables.\n",
        "* An optimization method which updates the variables of the model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "T5Szf8todNuU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Let's Start with a very Simple Example!"
      ]
    },
    {
      "metadata": {
        "id": "NPPDzhrLjxv0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xvtl0CGhddDU",
        "colab_type": "code",
        "outputId": "210bbb92-52fc-4f05-aa1b-90b1c6c1bf1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "cell_type": "code",
      "source": [
        "# first, create a TensorFlow constant\n",
        "const = tf.constant(2.0, name=\"const\")\n",
        "\n",
        "# # create TensorFlow variables\n",
        "# b = tf.Variable(2.0, name='b')\n",
        "\n",
        "# create TensorFlow variables\n",
        "b = tf.placeholder(tf.float32, [None, 1], name='b')\n",
        "\n",
        "\n",
        "c = tf.Variable(1.0, name='c')\n",
        "# now create some operations\n",
        "d = tf.add(b, c, name='d')\n",
        "e = tf.add(c, const, name='e')\n",
        "a = tf.multiply(d, e, name='a')\n",
        "\n",
        "# setup the variable initialisation\n",
        "init_op = tf.global_variables_initializer()\n",
        "\n",
        "# config = tf.ConfigProto()\n",
        "# config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    # initialise the variables\n",
        "    sess.run(init_op)\n",
        "    # compute the output of the graph\n",
        "    # a_out = sess.run(a)\n",
        "    a_out = sess.run(a, feed_dict={b: np.arange(0, 10)[:, np.newaxis]})\n",
        "    print(\"Variable a is {}\".format(a_out))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Variable a is [[ 3.]\n",
            " [ 6.]\n",
            " [ 9.]\n",
            " [12.]\n",
            " [15.]\n",
            " [18.]\n",
            " [21.]\n",
            " [24.]\n",
            " [27.]\n",
            " [30.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "K4X_Aj4Zh6iC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/TensorFlow-data-flow-graph.gif\" width=\"400\" height=\"500\" align=\"center\"/>\n",
        "  "
      ]
    },
    {
      "metadata": {
        "id": "TWapGbwhitET",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The animated data flows between different nodes in the graph are tensors which are multi-dimensional data arrays.  For instance, the input data tensor may be 5000 x 64 x 1, which represents a 64 node input layer with 5000 training samples.  After the input layer there is a hidden layer with rectified linear units as the activation function.  There is a final output layer (called a “logit layer” in the above graph) which uses cross entropy as a cost/loss function.  At each point we see the relevant tensors flowing to the “Gradients” block which finally flow to the Stochastic Gradient Descent optimiser which performs the back-propagation and gradient descent."
      ]
    },
    {
      "metadata": {
        "id": "efI5A37BdWb1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Now Let's See a NN Example!"
      ]
    },
    {
      "metadata": {
        "id": "RXZ6CZWUju_B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G5cg5WJdVlMr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<center><img src=\"https://cdn-images-1.medium.com/max/1600/1*9Mjoc_J0JR294YwHGXwCeg.jpeg\" width=\"400\" height=\"200\" align=\"center\"/>"
      ]
    },
    {
      "metadata": {
        "id": "3QTPaythi97c",
        "colab_type": "code",
        "outputId": "2b0fa84f-d0d6-420c-a91d-046f33805cb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "cell_type": "code",
      "source": [
        " # Read Data Set\n",
        "  mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "  # Python optimisation variables\n",
        "  learning_rate = 0.5\n",
        "  epochs = 10\n",
        "  batch_size = 100\n",
        "\n",
        "  # declare the training data placeholders\n",
        "  # input x - for 28 x 28 pixels = 784\n",
        "  x = tf.placeholder(tf.float32, [None, 784])\n",
        "  # now declare the output data placeholder - 10 digits\n",
        "  y = tf.placeholder(tf.float32, [None, 10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-4-75145abc0391>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "M14feYG2kIou",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Notice the x input layer is 784 nodes corresponding to the 28 x 28 (=784) pixels, and the y output layer is 10 nodes corresponding to the 10 possible digits.  "
      ]
    },
    {
      "metadata": {
        "id": "g0LylflLi-DW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "  # now declare the weights connecting the input to the hidden layer\n",
        "  W1 = tf.Variable(tf.random_normal([784, 300], stddev=0.03), name='W1')\n",
        "  b1 = tf.Variable(tf.random_normal([300]), name='b1')\n",
        "  # and the weights connecting the hidden layer to the output layer\n",
        "  W2 = tf.Variable(tf.random_normal([300, 10], stddev=0.03), name='W2')\n",
        "  b2 = tf.Variable(tf.random_normal([10]), name='b2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1d_inzWDlI0O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First, we declare some variables for W1 and b1, the weights and bias for the connections between the input and hidden layer.  This neural network will have 300 nodes in the hidden layer, so the size of the weight tensor W1 is [784, 300].  We initialise the values of the weights using a random normal distribution with a mean of zero and a standard deviation of 0.03. "
      ]
    },
    {
      "metadata": {
        "id": "VaS4nEmzi-Kz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " # calculate the output of the hidden layer\n",
        "  hidden_out = tf.add(tf.matmul(x, W1), b1)\n",
        "  hidden_out = tf.nn.relu(hidden_out)\n",
        "\n",
        "  # now calculate the hidden layer output - in this case, let's use a softmax activated\n",
        "  # output layer\n",
        "  y_ = tf.nn.softmax(tf.add(tf.matmul(hidden_out, W2), b2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B50hVPXcldZr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We finalise the hidden_out operation by applying a rectified linear unit activation function to the matrix multiplication plus bias.  Note that TensorFlow has a rectified linear unit activation already setup for us, tf.nn.relu.\n",
        "\n",
        "We use a softmax activation for the output layer – we can use the included TensorFlow softmax function tf.nn.softmax."
      ]
    },
    {
      "metadata": {
        "id": "X14MbTvDjO5j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "  # now let's define the cost function which we are going to train the model on\n",
        "  y_clipped = tf.clip_by_value(y_, 1e-10, 0.9999999)\n",
        "  cross_entropy = -tf.reduce_mean(tf.reduce_sum(y * tf.log(y_clipped)\n",
        "                                                + (1 - y) * tf.log(1 - y_clipped), axis=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5VN5jrcJlud5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We also have to include a cost or loss function for the optimisation / backpropagation to work on. Here we’ll use the cross entropy cost function. The first line is an operation converting the output y_ to a clipped version, limited between 1e-10 to 0.999999.  This is to make sure that we never get a case were we have a log(0) operation occurring during training – this would return NaN and break the training process."
      ]
    },
    {
      "metadata": {
        "id": "26o_ut6njRm8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " # add an optimiser\n",
        "  optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "APsxJNq-mUZ2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here we are just using the gradient descent optimiser provided by TensorFlow."
      ]
    },
    {
      "metadata": {
        "id": "a2SE39SlVL9F",
        "colab_type": "code",
        "outputId": "83ef343e-a8be-4634-b676-f374dab7aeaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "cell_type": "code",
      "source": [
        "  # finally setup the initialisation operator\n",
        "  init_op = tf.global_variables_initializer()\n",
        "\n",
        "  # define an accuracy assessment operation\n",
        "  correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "\n",
        "  # start the session\n",
        "  with tf.Session() as sess:\n",
        "      # initialise the variables\n",
        "      sess.run(init_op)\n",
        "      total_batch = int(len(mnist.train.labels) / batch_size)\n",
        "      for epoch in range(epochs):\n",
        "          avg_cost = 0\n",
        "          for i in range(total_batch):\n",
        "              batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)\n",
        "              o, c = sess.run([optimiser, cross_entropy], feed_dict={x: batch_x, y: batch_y})\n",
        "              # print(sess.run(b2))\n",
        "              avg_cost += c / total_batch\n",
        "          print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(avg_cost))\n",
        "\n",
        "\n",
        "      print(\"\\nTraining complete!\")\n",
        "\n",
        "      print(sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels}))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 cost = 0.598\n",
            "Epoch: 2 cost = 0.218\n",
            "Epoch: 3 cost = 0.156\n",
            "Epoch: 4 cost = 0.121\n",
            "Epoch: 5 cost = 0.101\n",
            "Epoch: 6 cost = 0.083\n",
            "Epoch: 7 cost = 0.066\n",
            "Epoch: 8 cost = 0.053\n",
            "Epoch: 9 cost = 0.043\n",
            "Epoch: 10 cost = 0.035\n",
            "\n",
            "Training complete!\n",
            "0.9777\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kh5_VVcCnC4U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Now Let's Try Some Convolutional NN with TensorFlow!"
      ]
    },
    {
      "metadata": {
        "id": "9dnD6aarnuXd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First thing, we will create a function that will create a convolutional NN layer for us. To do this, we will call the tf function *tf.nn.conv2d*:\n",
        "\n",
        "\n",
        "```\n",
        "# Conv2d Function Inputs:\n",
        "tf.nn.conv2d(\n",
        "    input,\n",
        "    filter,\n",
        "    strides,\n",
        "    padding,\n",
        "    use_cudnn_on_gpu=True,\n",
        "    name=None\n",
        ")\n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "VsBFkKGsj1Nv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_new_conv_layer(input_data, num_input_channels, num_filters, filter_shape, pool_shape, name):\n",
        "  \n",
        "    # setup the filter input shape for tf.nn.conv_2d\n",
        "    conv_filt_shape = [filter_shape[0], filter_shape[1], num_input_channels, num_filters]\n",
        "\n",
        "    # initialise weights and bias for the filter\n",
        "    weights = tf.Variable(tf.truncated_normal(conv_filt_shape, stddev=0.03), name=name+'_W')\n",
        "    bias = tf.Variable(tf.truncated_normal([num_filters]), name=name+'_b')\n",
        "\n",
        "    # setup the convolutional layer operation\n",
        "    out_layer = tf.nn.conv2d(input_data, weights, [1, 1, 1, 1], padding='SAME')\n",
        "\n",
        "    # add the bias\n",
        "    out_layer += bias\n",
        "\n",
        "    # apply a ReLU non-linear activation\n",
        "    out_layer = tf.nn.relu(out_layer)\n",
        "\n",
        "    # now perform max pooling\n",
        "   \n",
        "    ksize = [1, pool_shape[0], pool_shape[1], 1]   \n",
        "    strides = [1, 2, 2, 1]\n",
        "    \n",
        "    out_layer = tf.nn.max_pool(out_layer, ksize=ksize, strides=strides, padding='SAME')\n",
        "\n",
        "    return out_layer\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gzwmhl11o4Ce",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "'ksize' is the argument which defines the size of the max pooling window (i.e. the area over which the maximum is calculated).  It must be 4D to match the convolution - in this case, for each image we want to use a 2 x 2 area applied to each channel.\n",
        "\n",
        "'strides' defines how the max pooling area moves through the image - a stride of 2 in the x direction will lead to max pooling areas starting at x=0, x=2, x=4 etc. through your image.  If the stride is 1, we will get max pooling overlapping previous max pooling areas (and no reduction in the number of parameters).  In this case, we want to do strides of 2 in the x and y directions.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "wcY3q50n1tik",
        "colab_type": "code",
        "outputId": "68e1b0cc-0877-4f96-905a-298c6a67c6e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2FJjgjdCpHsd",
        "colab_type": "code",
        "outputId": "87049630-0faa-47ff-cf14-88d33d607759",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Python optimisation variables\n",
        "learning_rate = 0.0001\n",
        "epochs = 10\n",
        "batch_size = 50\n",
        "\n",
        "# declare the training data placeholders\n",
        "# input x - for 28 x 28 pixels = 784 - this is the flattened image data that is drawn from mnist.train.nextbatch()\n",
        "x = tf.placeholder(tf.float32, [None, 784])\n",
        "\n",
        "# reshape the input data so that it is a 4D tensor.  The first value (-1) tells function to dynamically shape that\n",
        "# dimension based on the amount of data passed to it.  The two middle dimensions are set to the image size (i.e. 28\n",
        "# x 28).  The final dimension is 1 as there is only a single colour channel i.e. grayscale.  If this was RGB, this\n",
        "# dimension would be 3\n",
        "\n",
        "x_shaped = tf.reshape(x, [-1, 28, 28, 1])\n",
        "# now declare the output data placeholder - 10 digits\n",
        "y = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "# create some convolutional layers\n",
        "layer1 = create_new_conv_layer(x_shaped, 1, 32, [5, 5], [2, 2], name='layer1')\n",
        "layer2 = create_new_conv_layer(layer1, 32, 64, [5, 5], [2, 2], name='layer2')\n",
        "\n",
        "# flatten the output ready for the fully connected output stage - after two layers of stride 2 pooling, we go\n",
        "# from 28 x 28, to 14 x 14 to 7 x 7 x,y co-ordinates, but with 64 output channels.  To create the fully connected,\n",
        "# \"dense\" layer, the new shape needs to be [-1, 7 x 7 x 64]\n",
        "flattened = tf.reshape(layer2, [-1, 7 * 7 * 64])\n",
        "\n",
        "# setup some weights and bias values for this layer, then activate with ReLU\n",
        "wd1 = tf.Variable(tf.truncated_normal([7 * 7 * 64, 1000], stddev=0.03), name='wd1')\n",
        "bd1 = tf.Variable(tf.truncated_normal([1000], stddev=0.01), name='bd1')\n",
        "dense_layer1 = tf.matmul(flattened, wd1) + bd1\n",
        "dense_layer1 = tf.nn.relu(dense_layer1)\n",
        "\n",
        "# another layer with softmax activations\n",
        "wd2 = tf.Variable(tf.truncated_normal([1000, 10], stddev=0.03), name='wd2')\n",
        "bd2 = tf.Variable(tf.truncated_normal([10], stddev=0.01), name='bd2')\n",
        "dense_layer2 = tf.matmul(dense_layer1, wd2) + bd2\n",
        "y_ = tf.nn.softmax(dense_layer2)\n",
        "\n",
        "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=dense_layer2, labels=y))\n",
        "\n",
        "# add an optimiser\n",
        "optimiser = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
        "\n",
        "# define an accuracy assessment operation\n",
        "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "# setup the initialisation operator\n",
        "init_op = tf.global_variables_initializer()\n",
        "\n",
        "# # setup recording variables\n",
        "# # add a summary to store the accuracy\n",
        "# tf.summary.scalar('accuracy', accuracy)\n",
        "\n",
        "# merged = tf.summary.merge_all()\n",
        "# writer = tf.summary.FileWriter('C:\\\\Users\\\\ameni\\\\Desktop\\\\csu\\\\Machine_Learning\\\\TF')\n",
        "with tf.Session() as sess:\n",
        "    # initialise the variables\n",
        "    sess.run(init_op)\n",
        "    total_batch = int(len(mnist.train.labels) / batch_size)\n",
        "    for epoch in range(epochs):\n",
        "        avg_cost = 0\n",
        "        for i in range(total_batch):\n",
        "            batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)\n",
        "            _, c = sess.run([optimiser, cross_entropy], feed_dict={x: batch_x, y: batch_y})\n",
        "            avg_cost += c / total_batch\n",
        "        test_acc = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
        "        print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(avg_cost), \" test accuracy: {:.3f}\".format(test_acc))\n",
        "#         summary = sess.run(merged, feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
        "#         writer.add_summary(summary, epoch)\n",
        "\n",
        "    print(\"\\nTraining complete!\")\n",
        "#     writer.add_graph(sess.graph)\n",
        "    print(sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels}))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-12-91578730d3e6>:39: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
            "\n",
            "Epoch: 1 cost = 0.661  test accuracy: 0.928\n",
            "Epoch: 2 cost = 0.167  test accuracy: 0.966\n",
            "Epoch: 3 cost = 0.103  test accuracy: 0.976\n",
            "Epoch: 4 cost = 0.075  test accuracy: 0.982\n",
            "Epoch: 5 cost = 0.060  test accuracy: 0.983\n",
            "Epoch: 6 cost = 0.051  test accuracy: 0.985\n",
            "Epoch: 7 cost = 0.042  test accuracy: 0.987\n",
            "Epoch: 8 cost = 0.036  test accuracy: 0.988\n",
            "Epoch: 9 cost = 0.031  test accuracy: 0.989\n",
            "Epoch: 10 cost = 0.027  test accuracy: 0.988\n",
            "\n",
            "Training complete!\n",
            "0.988\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VnTkUSLLdxAE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TensorBoard DEMO !!"
      ]
    },
    {
      "metadata": {
        "id": "gOGA7wjSswT5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Dataset Example"
      ]
    },
    {
      "metadata": {
        "id": "EsUleYEEsw9X",
        "colab_type": "code",
        "outputId": "7009a1f4-628b-4e49-8e22-4e27179deaff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "\n",
        "digits = load_digits(return_X_y=True)\n",
        "print(digits[0].shape)\n",
        "print(digits[1].shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1797, 64)\n",
            "(1797,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "J3xzMEvAs3yq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# split into train and validation sets\n",
        "  train_images = digits[0][:int(len(digits[0]) * 0.8)]\n",
        "  train_labels = digits[1][:int(len(digits[0]) * 0.8)]\n",
        "  valid_images = digits[0][int(len(digits[0]) * 0.8):]\n",
        "  valid_labels = digits[1][int(len(digits[0]) * 0.8):]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NW2XTgLA-Tll",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UjjKEgcrv5tP",
        "colab_type": "code",
        "outputId": "a3465a85-9c44-4b2f-880b-a1805af03b63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "# create the training datasets\n",
        "  dx_train = tf.data.Dataset.from_tensor_slices(train_images)\n",
        "  \n",
        "# Let's look at the type change of our data!\n",
        "  print(type(train_images))\n",
        "  print(type(dx_train))\n",
        "  \n",
        "  \n",
        "  # apply a one-hot transformation to each label for use in the neural network\n",
        "  dy_train = tf.data.Dataset.from_tensor_slices(train_labels).map(lambda z: tf.one_hot(z, 10))\n",
        "  # zip the x and y training data together and shuffle, batch etc.\n",
        "  train_dataset = tf.data.Dataset.zip((dx_train, dy_train)).shuffle(500).repeat().batch(30)\n",
        "  # do the same operations for the validation set\n",
        "  dx_valid = tf.data.Dataset.from_tensor_slices(valid_images)\n",
        "  dy_valid = tf.data.Dataset.from_tensor_slices(valid_labels).map(lambda z: tf.one_hot(z, 10))\n",
        "  valid_dataset = tf.data.Dataset.zip((dx_valid, dy_valid)).shuffle(500).repeat().batch(30)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "<class 'tensorflow.python.data.ops.dataset_ops.TensorSliceDataset'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nsB35T7Ayf7l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The training x and y data is zipped together in the full train_dataset. Chained along together with this zip method is first the *shuffle()* dataset method. This method randomly shuffles the data, using a buffer of data specified in the argument – 500 in this case. Next, the *repeat()* method is used, to allow the iterator to continuously extract data from this dataset. When this method (*repeat()*) is applied to the dataset with no argument, it means that the dataset can be repeated indefinitely without throwing an OutOfRangeError. Finally the data is batched with a batch size of 30."
      ]
    },
    {
      "metadata": {
        "id": "9kpj7cf_wP4e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "  # create general iterator\n",
        "  iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
        "                                             train_dataset.output_shapes)\n",
        "  next_element = iterator.get_next()\n",
        "  # make datasets that we can initialize separately, but using the same structure via the common iterator\n",
        "  training_init_op = iterator.make_initializer(train_dataset)\n",
        "  validation_init_op = iterator.make_initializer(valid_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ojpz9zm7zRqA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, we want to be able to extract data from either the train_dataset or the valid_dataset seamlessly. This is important, as we don’t want to have to change how data flows through the neural network structure when all we want to do is just change the dataset the model is consuming. To do this, we can use another way of creating the Iterator object – the from_structure() method. This method creates a generic iterator object – all it needs is the data types of the data it will be outputting and the output data size/shape in order to be created.\n",
        "\n",
        "The second line of the above creates a standard get_next() iterator operation which can be called to extract data from this generic iterator structure."
      ]
    },
    {
      "metadata": {
        "id": "pMSSF9qPz3EM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def nn_model(in_data):\n",
        "    bn = tf.layers.batch_normalization(in_data)\n",
        "    fc1 = tf.layers.dense(bn, 50)\n",
        "    fc2 = tf.layers.dense(fc1, 50)\n",
        "    fc2 = tf.layers.dropout(fc2)\n",
        "    fc3 = tf.layers.dense(fc2, 10)\n",
        "    return fc3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VaHSqHKTzZoA",
        "colab_type": "code",
        "outputId": "0ddcc849-fe9f-4a32-d855-158589ed61bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "cell_type": "code",
      "source": [
        "logits = nn_model(next_element[0])\n",
        "# add the optimizer and loss\n",
        "loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits_v2(labels=next_element[1], logits=logits))\n",
        "optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
        "# get accuracy\n",
        "prediction = tf.argmax(logits, 1)\n",
        "equality = tf.equal(prediction, tf.argmax(next_element[1], 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(equality, tf.float32))\n",
        "init_op = tf.global_variables_initializer()\n",
        "# run the training\n",
        "epochs = 600\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init_op)\n",
        "    sess.run(training_init_op)\n",
        "    for i in range(epochs):\n",
        "\n",
        "        l, _, acc = sess.run([loss, optimizer, accuracy])\n",
        "        if i % 50 == 0:\n",
        "            # print(sess.run(tf.shape(next_element[0])))\n",
        "            print(\"Epoch: {}, loss: {:.3f}, training accuracy: {:.2f}%\".format(i, l, acc * 100))\n",
        "    # now setup the validation run\n",
        "    valid_iters = 100\n",
        "    # re-initialize the iterator, but this time with validation data\n",
        "    sess.run(validation_init_op)\n",
        "    avg_acc = 0\n",
        "    for i in range(valid_iters):\n",
        "        acc = sess.run([accuracy])\n",
        "        avg_acc += acc[0]\n",
        "    print(\"Average validation set accuracy over {} iterations is {:.2f}%\".format(valid_iters,\n",
        "                                                                                 (avg_acc / valid_iters) * 100))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, loss: 475.750, training accuracy: 10.00%\n",
            "Epoch: 50, loss: 12.298, training accuracy: 90.00%\n",
            "Epoch: 100, loss: 4.910, training accuracy: 90.00%\n",
            "Epoch: 150, loss: 12.442, training accuracy: 90.00%\n",
            "Epoch: 200, loss: 1.859, training accuracy: 100.00%\n",
            "Epoch: 250, loss: 2.246, training accuracy: 93.33%\n",
            "Epoch: 300, loss: 3.609, training accuracy: 90.00%\n",
            "Epoch: 350, loss: 2.346, training accuracy: 96.67%\n",
            "Epoch: 400, loss: 1.726, training accuracy: 100.00%\n",
            "Epoch: 450, loss: 4.693, training accuracy: 93.33%\n",
            "Epoch: 500, loss: 3.151, training accuracy: 96.67%\n",
            "Epoch: 550, loss: 0.202, training accuracy: 100.00%\n",
            "Average validation set accuracy over 100 iterations is 90.13%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aPsSpj9vNsIv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Why TensorFlow Debugging is Difficult? "
      ]
    },
    {
      "metadata": {
        "id": "6RY-Fpr3NyYh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The concept of Computation Graph might be unfamiliar to us.\n",
        "* The \"Inversion of Control\"\n",
        "  * The actual computation (feed-forward, training) of model runs inside Session.run(), upon the computation graph, but not upon the Python code we wrote\n",
        "  * What is exactly being done during an execution of session is under an abstraction barrier\n",
        "* Therefore, we do not retrieve the intermediate values during the computation, unless we explicitly fetch them via Session.run()"
      ]
    },
    {
      "metadata": {
        "id": "h-XLNMf6O7Y1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Debugging in TensorFlow:\n",
        "\n",
        "* Basic ways:\n",
        "\n",
        "  * Explicitly fetch, and print (or do whatever you want)! \\\\\n",
        "      Session.run()\n",
        "  * Tensorboard: Histogram and Image Summary\n",
        "  * the tf.Print() operation\n",
        "* Advanced ways:\n",
        "\n",
        "  * A step-by-step debugger (ipdb, pdb, pudb ...)\n",
        "  * tfdbg: The TensorFlow debugger\n",
        "  * Tensorboard Debugger"
      ]
    }
  ]
}