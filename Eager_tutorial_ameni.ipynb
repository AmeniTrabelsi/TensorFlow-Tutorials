{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Eager-tutorial-ameni.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "FzI7Rr_sK8lN"
      },
      "cell_type": "markdown",
      "source": [
        "The **eager execution** for TensorFlow is a low-level interface allowing a more dynamic programming experience. Eager execution greatly simplifies how you can write and debug models, softening the complete separation between the *definition* of the operations and their *execution* in the standard TensorFlow interface."
      ]
    },
    {
      "metadata": {
        "id": "rMuHuxdAybUn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Eager execution is a flexible machine learning platform for research and experimentation, providing:\n",
        "-  An intuitive interface—Structure your code naturally and use Python data structures. Quickly iterate on small models and small data.\n",
        "-  Easier debugging—Call ops directly to inspect running models and test changes. Use standard Python debugging tools for immediate error reporting.\n",
        "-  Natural control flow—Use Python control flow instead of graph control flow, simplifying the specification of dynamic models.\n",
        "    "
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "JOX81EBYEfJ6"
      },
      "cell_type": "markdown",
      "source": [
        "# Static and dynamic graph computation"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ZX1Ad2mcEsE4"
      },
      "cell_type": "markdown",
      "source": [
        "To understand the need for eager execution, consider a simple example:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "bOZtwTAcEbHc",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "a = tf.constant(3.0)\n",
        "b = a + 2.0\n",
        "print(b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "lQatWScWHiNh"
      },
      "cell_type": "markdown",
      "source": [
        "If you never played with the low-level components of TensorFlow before, you probably would have expected the print operation to show the value of `b` at this point. Instead, we have to fetch the value of the variable by running the operation inside a `Session` object:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "rwhQ5L1zGi17",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sess = tf.Session()\n",
        "with sess.as_default():\n",
        "  print(sess.run(b))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "qAFSauM9nKwN"
      },
      "cell_type": "markdown",
      "source": [
        "# Enabling eager execution"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ysEhDOAdncOI"
      },
      "cell_type": "markdown",
      "source": [
        "Eager was introduced experimentally in TensorFlow v1.5, but in order to use all functionalities, we need to install the latest version (v1.7rc0 as of this writing). "
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "4uG9v_14npop"
      },
      "cell_type": "markdown",
      "source": [
        "Eager is enabled with a single line:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jfP0tOofnJ9S",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.contrib.eager as tfe\n",
        "import numpy as np\n",
        "tfe.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "8azE_xwVr4ak"
      },
      "cell_type": "markdown",
      "source": [
        "## Variables and gradients with eager execution"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "2lkQTVjyiHN-"
      },
      "cell_type": "markdown",
      "source": [
        "### Eager variables and NumPy compatibility"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "X1REd6l0szLs"
      },
      "cell_type": "markdown",
      "source": [
        "Eager execution works nicely with NumPy. NumPy operations accept tf.Tensor arguments. TensorFlow math operations convert Python objects and NumPy arrays to tf.Tensor objects. The tf.Tensor.numpy method returns the object's value as a NumPy ndarray.\n",
        "\n",
        "Before going straight to the model definition, let us see how variables and gradients are handled under the eager execution. First, eager comes with its own implementation of variables, which are automatically initalized when requested:"
      ]
    },
    {
      "metadata": {
        "id": "E_AyvYy-ybVj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "a = tf.constant([[1, 2],\n",
        "                 [3, 4]])\n",
        "print(a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bXiXSC3JybVx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Broadcasting support\n",
        "b = tf.add(a, 1)\n",
        "print(b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I5UyNlyoybV1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Operator overloading is supported\n",
        "print(a * b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tEnzcGU3ybV4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Use NumPy values\n",
        "import numpy as np\n",
        "\n",
        "c = np.multiply(a, b)\n",
        "print(c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_XJ3OqIiybWF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Obtain numpy value from a tensor:\n",
        "print(a.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xklb8cNuybWJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Dynamic control flow"
      ]
    },
    {
      "metadata": {
        "id": "G2QpgcQBybWK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A major benefit of eager execution is that all the functionality of the host language is available while your model is executing. So, for example, it is easy to write fizzbuzz:"
      ]
    },
    {
      "metadata": {
        "id": "21aADlmTybWY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def fizzbuzz(max_num):\n",
        "    counter = tf.constant(0)\n",
        "    max_num = tf.convert_to_tensor(max_num)\n",
        "    for num in range(max_num.numpy()):\n",
        "        num = tf.constant(num)\n",
        "        if int(num % 3) == 0 and int(num % 5) == 0:\n",
        "            print('FizzBuzz')\n",
        "        elif int(num % 3) == 0:\n",
        "            print('Fizz')\n",
        "        elif int(num % 5) == 0:\n",
        "            print('Buzz')\n",
        "        else:\n",
        "            print(num)\n",
        "        counter += 1\n",
        "    return counter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tXMsqMpoybWf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This has conditionals that depend on tensor values and it prints these values at runtime."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "VtfqaWPyiLJG"
      },
      "cell_type": "markdown",
      "source": [
        "### Computing gradients in eager"
      ]
    },
    {
      "metadata": {
        "id": "V17ykgXWybWh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Automatic differentiation is useful for implementing machine learning algorithms such as backpropagation for training neural networks. During eager execution, use tf.GradientTape to trace operations for computing gradients later."
      ]
    },
    {
      "metadata": {
        "id": "iButcp4gybWj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "tf.GradientTape is an opt-in feature to provide maximal performance when not tracing. Since different operations can occur during each call, all forward-pass operations get recorded to a \"tape\". To compute the gradient, play the tape backwards and then discard. A particular tf.GradientTape can only compute one gradient; subsequent calls throw a runtime error."
      ]
    },
    {
      "metadata": {
        "id": "xhSPQTp9ybWl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "w = tfe.Variable([[1.0]])\n",
        "with tfe.GradientTape() as tape:\n",
        "    loss = w * w\n",
        "\n",
        "grad = tape.gradient(loss, [w])\n",
        "print(grad) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h8Um8Ca9ybWn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here's an example of tf.GradientTape that records forward-pass operations to train a simple model:"
      ]
    },
    {
      "metadata": {
        "id": "BhmK5UMyybWp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# A toy dataset of points around 3 * x + 2\n",
        "NUM_EXAMPLES = 1000\n",
        "training_inputs = tf.random_normal([NUM_EXAMPLES])\n",
        "noise = tf.random_normal([NUM_EXAMPLES])\n",
        "training_outputs = training_inputs * 3 + 2 + noise\n",
        "\n",
        "def prediction(input, weight, bias):\n",
        "  return input * weight + bias\n",
        "\n",
        "# A loss function using mean-squared error\n",
        "def loss(weights, biases):\n",
        "  error = prediction(training_inputs, weights, biases) - training_outputs\n",
        "  return tf.reduce_mean(tf.square(error))\n",
        "\n",
        "# Return the derivative of loss with respect to weight and bias\n",
        "def grad(weights, biases):\n",
        "  with tfe.GradientTape() as tape:\n",
        "    loss_value = loss(weights, biases)\n",
        "  return tape.gradient(loss_value, [weights, biases])\n",
        "\n",
        "train_steps = 200\n",
        "learning_rate = 0.01\n",
        "# Start with arbitrary values for W and B on the same batch of data\n",
        "W = tfe.Variable(5.)\n",
        "B = tfe.Variable(10.)\n",
        "\n",
        "print(\"Initial loss: {:.3f}\".format(loss(W, B)))\n",
        "\n",
        "for i in range(train_steps):\n",
        "  dW, dB = grad(W, B)\n",
        "  W.assign_sub(dW * learning_rate)\n",
        "  B.assign_sub(dB * learning_rate)\n",
        "  if i % 20 == 0:\n",
        "    print(\"Loss at step {:03d}: {:.3f}\".format(i, loss(W, B)))\n",
        "\n",
        "print(\"Final loss: {:.3f}\".format(loss(W, B)))\n",
        "print(\"W = {}, B = {}\".format(W.numpy(), B.numpy()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4QN5V62NybWu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "tfe.Variable objects store mutable tf.Tensor values accessed during training to make automatic differentiation easier. The parameters of a model can be encapsulated in classes as variables."
      ]
    },
    {
      "metadata": {
        "id": "fyn4gChQybWw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "class Model(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.W = tfe.Variable(5., name='weight')\n",
        "    self.B = tfe.Variable(10., name='bias')\n",
        "  def call(self, inputs):\n",
        "    return inputs * self.W + self.B\n",
        "\n",
        "# A toy dataset of points around 3 * x + 2\n",
        "NUM_EXAMPLES = 2000\n",
        "training_inputs = tf.random_normal([NUM_EXAMPLES])\n",
        "noise = tf.random_normal([NUM_EXAMPLES])\n",
        "training_outputs = training_inputs * 3 + 2 + noise\n",
        "\n",
        "# The loss function to be optimized\n",
        "def loss(model, inputs, targets):\n",
        "  error = model(inputs) - targets\n",
        "  return tf.reduce_mean(tf.square(error))\n",
        "\n",
        "def grad(model, inputs, targets):\n",
        "  with tfe.GradientTape() as tape:\n",
        "    loss_value = loss(model, inputs, targets)\n",
        "  return tape.gradient(loss_value, [model.W, model.B])\n",
        "\n",
        "# Define:\n",
        "# 1. A model.\n",
        "# 2. Derivatives of a loss function with respect to model parameters.\n",
        "# 3. A strategy for updating the variables based on the derivatives.\n",
        "model = Model()\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "\n",
        "print(\"Initial loss: {:.3f}\".format(loss(model, training_inputs, training_outputs)))\n",
        "\n",
        "# Training loop\n",
        "for i in range(300):\n",
        "  grads = grad(model, training_inputs, training_outputs)\n",
        "  optimizer.apply_gradients(zip(grads, [model.W, model.B]),\n",
        "                            global_step=tf.train.get_or_create_global_step())\n",
        "  if i % 20 == 0:\n",
        "    print(\"Loss at step {:03d}: {:.3f}\".format(i, loss(model, training_inputs, training_outputs)))\n",
        "\n",
        "print(\"Final loss: {:.3f}\".format(loss(model, training_inputs, training_outputs)))\n",
        "print(\"W = {}, B = {}\".format(model.W.numpy(), model.B.numpy()))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}